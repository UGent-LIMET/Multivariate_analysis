# Title: Software framework for the processing and statistical analysis of multivariate MS-data
# Owner: Laboratory of Integrative Metabolomics (LIMET), Ghent university
# Creator: Dr. Marilyn De Graeve
# Maintainer: <limet@ugent.be>
# Script: hyperparameter optimization statistics


##########R Pipeline - Part II: Hyperparameter optimization statistics##########
print(Sys.time())
start_time <- Sys.time()
print("R pipeline - Part II: Hyperparameter optimization statistics - start!")
# Part II: Hyperparameter optimization statistics



#todo exapnd to pareto, log in final stat analysis...
TRANSFORMATION_NOT <- 'no transformation'
TRANSFORMATION_LOG <- 'Log transformation' #natural log with (x+1) so no inf

SCALING_NOT <- 'no scaling'
SCALING_PARETO <- 'Pareto scaling'

TRANSFORMATION <- c(TRANSFORMATION_LOG) #TRANSFORMATION_NOT, 
SCALING <- c(SCALING_PARETO) #SCALING_NOT, 



###repeat necessary steps of part 2: statisitcal analysis (incl. variable filtering)
########merge, skew, variable selection############
## data_loading
setwd(path_data_in)
COLLUMN_NR_COMPARISON1 <- COLLUMN_NR_LAST_BEFORE_COMPARISONS + 1        #column number of the column 'Comparison1' for pairwise comparison (OPLSDA)
COLLUMN_NR_MULTIPLE_COMPARISON1 <- COLLUMN_NR_COMPARISON1 + AMOUNT_OF_COMPARISONS         #column number of the column 'MultipleComparison1' for multiple comparison (PLSDA, LIMMA)
COLLUMN_NR_PROJECTION1 <- COLLUMN_NR_MULTIPLE_COMPARISON1 + AMOUNT_OF_MULTIPLE_COMPARISONS
COLLUMN_NR_START_VARIABLES <- COLLUMN_NR_PROJECTION1 + AMOUNT_OF_PROJECTIONS

sampleMetadata <- load_sampleMetadata(INPUT_SAMPLES)
#Add "X" to sampleNames
sampleMetadata[,1] <- paste(replicate(nrow(sampleMetadata),"X"), sampleMetadata[,1], sep="")
check_nrow_SM <- nrow(sampleMetadata)

if (INPUT_VARIABLES == VARIABLEMETADATA_FROM_PIPELINE){
  INPUT_VARIABLES <- paste(name_project, '_variableMetadata.txt', sep="")
  if(exists("COLLUMN_NR_START_SAMPLES") == FALSE){ 		#if after merge, will be given value 21, so do not change
    COLLUMN_NR_START_SAMPLES <- 20  #always 20 (auto and manual must be same format)
  }
}
variableMetadata <- load_variableMetadata(INPUT_VARIABLES)
#note: automatically adds X to samples at this point


## set directory to output/param 
dir.create(file.path(path_data_out, 'param'))
path_data_out_param <- file.path(path_data_out, 'param')
setwd(path_data_out_param)


## merge variables from variableMetadata into the sampleMetadata
### make sampleMatrix
library(data.table)
variableMetadata_from_start_samples <- subset(variableMetadata, select = -c(2:(COLLUMN_NR_START_SAMPLES-1))) #remove info exept CompID
sampleMatrix <- transpose(variableMetadata_from_start_samples)
colnames(sampleMatrix) <- variableMetadata_from_start_samples[ ,1]
sampleMatrix <- as.data.frame(sapply(sampleMatrix, as.numeric))
sampleMatrix$SampleName <- colnames(variableMetadata_from_start_samples)
sampleMatrix <- sampleMatrix[-1,] #remove first row (colnames compids + samplename; so 1 extra col than no of variables), is captured in colnames
#write_dataframe_as_txt_file(sampleMatrix, 'sampleMatrix.txt')

#merge variable intensities from compIDs to correct sample (if order not same), output order is sorted by samplename
sampleMetadata <- merge_accoding_to_SampleName(sampleMetadata, sampleMatrix)
write_dataframe_as_txt_file(sampleMetadata, 'sampleMetadata_variableMatrix_merged.txt')


##Check sampleMatrx 
#see if same number of rows (amount of sampleNames) as when loaded prev, so merge was succesfull
if(nrow(sampleMetadata) != check_nrow_SM){
  stop("ERROR: Part II: statistical analysis stopped because SampleMetadata and VariableMetadata are incompatible to merge into correct sampleMatrix.")
} 


### start hyperparameter optimization
#########hyperparameter###########
# method: gridsearch method (compare all combinations)
# todo: if needed in future, do with random optimization instead of all combi's

#need to start from original SM every loop:
save_sampleMetadata <- sampleMetadata

#skip combi's QC normalisation and QC-related filters instead of retrieving error if no QC in SM:
if(!("QC" %in% sampleMetadata$Type | "IQC" %in% sampleMetadata$Type)){
  COEFICIENT_OF_VARIANCE <- c(COEFICIENT_OF_VARIANCE_0) #instead of vector with options described above
  QC_PREFILTER <- c(NO_FILTER_QC_POOL) #instead of vector with options described above
  NORMALIZE_METHOD <- NORMALIZE_NOT #instead of vector with options described above
}

## list combinations of hyperparameters to optimize
report_hyperparameter_test <- NULL
combi_nr <-1
amount_of_filter1 <- length(COEFICIENT_OF_VARIANCE)
amount_of_filter2 <- length(QC_PREFILTER)
amount_of_is_param <- length(NORMALIZE_METHOD0)
amount_of_tic_param <- length(NORMALIZE_METHOD1)
amount_of_qc_param <- length(NORMALIZE_METHOD)
amount_of_transf_param <- length(TRANSFORMATION)
amount_of_scaling_param <- length(SCALING)


for(filter1 in 1:amount_of_filter1){
  for(filter2 in 1:amount_of_filter2){
  
    #need to start from original SM every loop: #start from merged SM
    sampleMetadata <- save_sampleMetadata
    
    #filter1 <- 1
    #filter2 <- 1
    #combi_nr <-1
    #print(COEFICIENT_OF_VARIANCE[filter1])
    #print(QC_PREFILTER[filter2])
    
    
    ### variable filtering
    ## optional: Delete variables with coeficent of variation of QCs > 30%
    if(COEFICIENT_OF_VARIANCE[filter1] == COEFICIENT_OF_VARIANCE_30){
      stopifnot("QC" %in% sampleMetadata$Type |"IQC" %in% sampleMetadata$Type)
      QC_metadata <- sampleMetadata[sampleMetadata$Type == 'QC' | sampleMetadata$Type == 'IQC',] 
      
      CV_values <- apply(QC_metadata[,COLLUMN_NR_START_VARIABLES:length(QC_metadata)], 2, function(x) sd(x)/mean(x))
      matrix_with_CV_values <- data.frame(x=colnames(QC_metadata)[COLLUMN_NR_START_VARIABLES:length(QC_metadata)],
                                          y=CV_values)
      names(matrix_with_CV_values) <- c("CompID", "CV_values")
      retained_variables_CV <- matrix_with_CV_values[(matrix_with_CV_values[,2]<=0.3),]
      retained_variables_CV <- retained_variables_CV[,1]
      
      all_matrix <- from_df_to_matrix(sampleMetadata)
      all_matrix <- as.data.frame(all_matrix)
      
      matrix_filter_coeficient_variance <- all_matrix[, names(all_matrix) %in% (retained_variables_CV)]
      
      sampleMetadata_until_start_variables <- subset(sampleMetadata, select = c(1:(COLLUMN_NR_START_VARIABLES-1)))
      sampleMetadata_filter_coeficient_variance <- cbind(sampleMetadata_until_start_variables, matrix_filter_coeficient_variance)
      #write_dataframe_as_txt_file(sampleMetadata_filter_coeficient_variance, paste0('sampleMetadata_filter_coeficient_variance', combi_nr, '.txt'))
      
      sampleMetadata <- sampleMetadata_filter_coeficient_variance
    }
    if(COEFICIENT_OF_VARIANCE[filter1] == COEFICIENT_OF_VARIANCE_0){
      sampleMetadata <- sampleMetadata
    }
    
    
    ## optional: retain variable in present in 80% of the QCs samples
    if(QC_PREFILTER[filter2] == FILTER_QC_POOL_80){
      stopifnot("QC" %in% sampleMetadata$Type |"IQC" %in% sampleMetadata$Type)
      QC_metadata <- sampleMetadata[sampleMetadata$Type == 'QC' | sampleMetadata$Type == 'IQC',] 
      number_QCs <- nrow(QC_metadata)
      perc_present <- colSums(QC_metadata[,COLLUMN_NR_START_VARIABLES:length(QC_metadata)] != 0)/number_QCs
      matrix_with_perc_present <- data.frame(x=colnames(QC_metadata)[COLLUMN_NR_START_VARIABLES:length(QC_metadata)],
                                             y=perc_present)
      names(matrix_with_perc_present) <- c("CompID", "perc_present")
      retained_variables_QCpool <- matrix_with_perc_present[(matrix_with_perc_present[,2]>=0.8),]
      retained_variables_QCpool <- retained_variables_QCpool[,1] #compIDs of good ones
      
      all_matrix <- from_df_to_matrix(sampleMetadata)
      all_matrix <- as.data.frame(all_matrix)
      
      matrix_filter_QCpool <- all_matrix[, names(all_matrix) %in% (retained_variables_QCpool)]
      
      sampleMetadata_until_start_variables <- subset(sampleMetadata, select = c(1:(COLLUMN_NR_START_VARIABLES-1)))
      sampleMetadata_filter_QCpool <- cbind(sampleMetadata_until_start_variables, matrix_filter_QCpool)
      #write_dataframe_as_txt_file(sampleMetadata_filter_QCpool, paste0('sampleMetadata_filter_QCpool', combi_nr, '.txt'))
      
      sampleMetadata <- sampleMetadata_filter_QCpool
    }
    if(QC_PREFILTER[filter2] == NO_FILTER_QC_POOL){
      sampleMetadata <- sampleMetadata
    }
   
    
    #########skew###########
    ## check if data has a normal distribution per Type(s)
    ### Check normality of all variables across all the samples (blanco, QC, STD, sample)
    all_matrix <- from_df_to_matrix(sampleMetadata)
    
    amount_of_variables_with_normal_distribution <- Shapiro_Wilk_test(all_matrix)
    total_amount_of_variables <- ncol(all_matrix)
    percentage <- round(((amount_of_variables_with_normal_distribution/total_amount_of_variables)*100), digits=2)
    
    report_check_normality <- paste("There are " , amount_of_variables_with_normal_distribution, " variables retrieved with a normal distribution from the total of " , total_amount_of_variables , " variables across all the samples (", percentage, "%).", sep="")
    name_report_normality <- paste(name_project,'_Report_normalities.txt', sep="")
    append_result_to_report(report_check_normality, name_report_normality)
    
    skewkurtplot <- plot_skewkurt(all_matrix)
    #pdf(paste(name_project,'_skewkurtplot_all', combi_nr, '.pdf', sep=""),height=5,width=7)
    #plot(skewkurtplot)
    #dev.off()
    
    png(paste(name_project,'_skewkurtplot_all', combi_nr, '.png', sep=""), width=7, height=5, units="in", res=150)
    plot(skewkurtplot)
    dev.off()
    
    ### Check normality of all variables across all biological samples + QC or IQC
    samples_QC_metadata <- sampleMetadata[sampleMetadata$Type == 'Sample' | sampleMetadata$Type == 'QC' | sampleMetadata$Type == 'IQC',] 
    samples_QC_matrix <- from_df_to_matrix(samples_QC_metadata)
    
    amount_of_variables_with_normal_distribution <- Shapiro_Wilk_test(samples_QC_matrix)
    total_amount_of_variables <- ncol(samples_QC_matrix)
    percentage <- round(((amount_of_variables_with_normal_distribution/total_amount_of_variables)*100), digits=2)
    
    report_check_normality <- paste("There are " , amount_of_variables_with_normal_distribution, " variables retrieved with a normal distribution from the total of " , total_amount_of_variables , " variables across the samples and QCs (", percentage, "%).", sep="")
    append_result_to_report(report_check_normality, name_report_normality)
    
    skewkurtplot <- plot_skewkurt(samples_QC_matrix)
    #pdf(paste(name_project,'_skewkurtplot_samples_QC', combi_nr, '.pdf', sep=""),height=5,width=7)
    #plot(skewkurtplot)
    #dev.off()
    
    png(paste(name_project,'_skewkurtplot_samples_QC', combi_nr, '.png', sep=""), width=7, height=5, units="in", res=150)
    plot(skewkurtplot)
    dev.off()
    
    ### Check normality of the biological samples
    samples_metadata <- sampleMetadata[sampleMetadata$Type == 'Sample',] 
    samples_matrix <- from_df_to_matrix(samples_metadata)
    
    amount_of_variables_with_normal_distribution <- Shapiro_Wilk_test(samples_matrix)
    total_amount_of_variables <- ncol(samples_matrix)
    percentage <- round(((amount_of_variables_with_normal_distribution/total_amount_of_variables)*100), digits=2)
    
    report_check_normality <- paste("There are " , amount_of_variables_with_normal_distribution, " variables retrieved with a normal distribution from the total of " , total_amount_of_variables , " variables across the biological samples (", percentage, "%).", sep="")
    append_result_to_report(report_check_normality, name_report_normality)
    
    skewkurtplot <- plot_skewkurt(samples_matrix)
    #pdf(paste(name_project,'_skewkurtplot_samples', combi_nr, '.pdf', sep=""),height=5,width=7)
    #plot(skewkurtplot)
    #dev.off()
    
    png(paste(name_project,'_skewkurtplot_samples', combi_nr, '.png', sep=""), width=7, height=5, units="in", res=150)
    plot(skewkurtplot)
    dev.off()
    
    ### Check ISTD = internal standards 
    ISTD_metadata <- sampleMetadata[sampleMetadata$Type == 'ISTD',]
    if(nrow(ISTD_metadata) == 0){
      report_check_normality <- paste("There are no ISTDs present to check the normality for.", sep="")
      append_result_to_report(report_check_normality, name_report_normality)
    }
    if(nrow(ISTD_metadata) > 0){
      ISTD_matrix <- from_df_to_matrix(ISTD_metadata)
      
      amount_of_variables_with_normal_distribution <- Shapiro_Wilk_test(ISTD_matrix)
      total_amount_of_variables <- ncol(ISTD_matrix)
      percentage <- round(((amount_of_variables_with_normal_distribution/total_amount_of_variables)*100), digits=2)
      
      report_check_normality <- paste("There are " , amount_of_variables_with_normal_distribution, " variables retrieved with a normal distribution from the total of " , total_amount_of_variables , " variables across the ISTDs (", percentage, "%).", sep="")
      append_result_to_report(report_check_normality, name_report_normality)
      
      skewkurtplot <- plot_skewkurt(ISTD_matrix)
      #pdf(paste(name_project,'_skewkurtplot_ISTD', combi_nr, '.pdf', sep=""),height=5,width=7)
      #plot(skewkurtplot)
      #dev.off()
      
      png(paste(name_project,'_skewkurtplot_ISTD', combi_nr, '.png', sep=""), width=7, height=5, units="in", res=150)
      plot(skewkurtplot)
      dev.off()
    }
    
    ### Check STD = internal standard mixture 
    STD_metadata <- sampleMetadata[sampleMetadata$Type == 'STD',]
    if(nrow(STD_metadata) == 0){
      report_check_normality <- paste("There are no STDs present to check the normality for.", sep="")
      append_result_to_report(report_check_normality, name_report_normality)
    }
    if(nrow(STD_metadata) > 0){
      STD_matrix <- from_df_to_matrix(STD_metadata)
      
      amount_of_variables_with_normal_distribution <- Shapiro_Wilk_test(STD_matrix)
      total_amount_of_variables <- ncol(STD_matrix)
      percentage <- round(((amount_of_variables_with_normal_distribution/total_amount_of_variables)*100), digits=2)
      
      report_check_normality <- paste("There are " , amount_of_variables_with_normal_distribution, " variables retrieved with a normal distribution from the total of " , total_amount_of_variables , " variables across the STDs (", percentage, "%).", sep="")
      append_result_to_report(report_check_normality, name_report_normality)
      
      skewkurtplot <- plot_skewkurt(STD_matrix)
      #pdf(paste(name_project,'_skewkurtplot_STD', combi_nr, '.pdf', sep=""),height=5,width=7)
      #plot(skewkurtplot)
      #dev.off()
      
      png(paste(name_project,'_skewkurtplot_STD', combi_nr, '.png', sep=""), width=7, height=5, units="in", res=150)
      plot(skewkurtplot)
      dev.off()
    }
    
    ### Check normality of QCs or IQCs
    QC_metadata <- sampleMetadata[sampleMetadata$Type == 'QC' | sampleMetadata$Type == 'IQC',] 
    QC_matrix <- from_df_to_matrix(QC_metadata)
    
    if(nrow(QC_matrix) == 0){
      report_check_normality <- paste("There are no (I)QCs present to check the normality for.", sep="")
      append_result_to_report(report_check_normality, name_report_normality)
    }
    if(nrow(QC_matrix) > 0){
      amount_of_variables_with_normal_distribution <- Shapiro_Wilk_test(QC_matrix)
      total_amount_of_variables <- ncol(QC_matrix)
      percentage <- round(((amount_of_variables_with_normal_distribution/total_amount_of_variables)*100), digits=2)
      
      report_check_normality <- paste("There are " , amount_of_variables_with_normal_distribution, " variables retrieved with a normal distribution from the total of " , total_amount_of_variables , " variables across the QCs (", percentage, "%).", sep="")
      append_result_to_report(report_check_normality, name_report_normality)
      
      skewkurtplot <- plot_skewkurt(QC_matrix)
      #pdf(paste(name_project,'_skewkurtplot_QC', combi_nr, '.pdf', sep=""),height=5,width=7)
      #plot(skewkurtplot)
      #dev.off()
      
      png(paste(name_project,'_skewkurtplot_QC', combi_nr, '.png', sep=""), width=7, height=5, units="in", res=150)
      plot(skewkurtplot)
      dev.off()
    }
    ####################
    
    
    
    #########loop tic, loop qc###########
    #need to start from original SM every loop: #changed since log/pareto so re-save for next loop
    save_sampleMetadataLogPar <- sampleMetadata

    for(isparam in 1:amount_of_is_param){
      for(ticparam in 1:amount_of_tic_param){
        for(param in 1:amount_of_qc_param){
          
          #need to start from original SM every loop: #changed since log/pareto
          sampleMetadata <- save_sampleMetadataLogPar
          
          #isparam <- 1
          #ticparam <- 1  #for testing
          #param <- 1   #for testing
          #print(combi_nr)
          #print(NORMALIZE_METHOD0[isparam])
          #print(NORMALIZE_METHOD1[ticparam])
          #print(NORMALIZE_METHOD[param])
            
          # individual loops per type of hyperparameter (is, tic, qc)
          
          ##########Normalisation##########
          
          ## Perform 0th normalisation with IS (optional)
          if(NORMALIZE_METHOD0[isparam] == NORMALIZE_WITH_IS){ 
            #IS or internal standard (either spicked in sample or added in solvent flow instrument
            #= intensity compID 1 / intenstity Comp IS for 1 sample/spectrum
            
            #check needed input (mz value IS) ok
            stopifnot(class(IS_MZ) == "numeric") #only if present and number
            stopifnot(class(PPM) == "numeric") #only if present and number
            
            #find CompID nr in VM to use below in SM
            ppm <- PPM
            standard <- IS_MZ
            standard_down <- standard - ((ppm*standard)/1000000) 
            standard_up <- standard + ((ppm*standard)/1000000)
            variableMetadata_standard <- variableMetadata[variableMetadata$MZ >= standard_down & variableMetadata$MZ <= standard_up, ]
            
            #if >=1 found, 1select closest to mz
            if(nrow(variableMetadata_standard) > 1){  
              ppm <- ppm/2 #half ppm and retry
              standard_down <- standard - ((ppm*standard)/1000000) 
              standard_up <- standard + ((ppm*standard)/1000000)
              variableMetadata_standard <- variableMetadata[variableMetadata$MZ >= standard_down & variableMetadata$MZ <= standard_up, ]
            }
            #if no found, stop script
            if(nrow(variableMetadata_standard) == 0){  
              stop("Error: IS was not found in variableMetadata. Check presence and adjust ppm")
            }
            compid_standard <- as.numeric(variableMetadata_standard$CompID)
            
            #at end all Intensties from IS will be =1 (devide by itself per sample) = 1#col in sm with IS
            averageIS_before <- mean(as.matrix(sampleMetadata[,(COLLUMN_NR_START_VARIABLES+compid_standard-1)])) #nr compid = 1-2-3, so same + info from Sm before
            
            #devide by vector with IS intensities
            IS_intensities <- sampleMetadata[,(COLLUMN_NR_START_VARIABLES+compid_standard-1)]
            
            #slow but sure correct way devide I/I from IS per samplerow
            matrix <- NULL
            for(rownr in 1:nrow(sampleMetadata)){
              rowvector <- sampleMetadata[rownr,COLLUMN_NR_START_VARIABLES:length(sampleMetadata)] / IS_intensities[rownr]
              matrix <- rbind(matrix, rowvector)
            }
            sampleMetadata[,COLLUMN_NR_START_VARIABLES:length(sampleMetadata)] <- matrix
            
            #must be re-scaled to same order of scale (afrondingsfouten bij kleine getallen tijdens berek PCA, daarom all getallen maal zelfde factor)
            averageIS_after <- mean(as.matrix(sampleMetadata[,(COLLUMN_NR_START_VARIABLES+compid_standard-1)]))
            factor <- averageIS_before/averageIS_after #average after is always 1, so just *average before (but good checkpoint)
            sampleMetadata[,COLLUMN_NR_START_VARIABLES:length(sampleMetadata)] <- factor * sampleMetadata[,COLLUMN_NR_START_VARIABLES:length(sampleMetadata)]
          }
          if(NORMALIZE_METHOD0[isparam] == NORMALIZE_NOT0){
            sampleMetadata <- sampleMetadata #nothing changes
          }
          #write_dataframe_as_txt_file(sampleMetadata, paste0('NORMALIZE_METHOD0_sampleMetadata', combi_nr, '.txt'))
          
          
          ## Perform 1th normalisation with TIC (optional)
          if(NORMALIZE_METHOD1[ticparam] == NORMALIZE_WITH_TIC){ 
            #TIC or total ion count normalisation accord to literature
            #https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3124646/
            #this normalization will be based on the sum of all intensity values in the mass spectrum (i.e., TIC)
            #= intensity compID 1 / sum(intenstity alls compIDs) for 1 sample/spectrum
            averageTIC_before <- mean(as.matrix(sampleMetadata[,COLLUMN_NR_START_VARIABLES:length(sampleMetadata)])) #sampleMetadata[,COLLUMN_NR_START_VARIABLES:length(sampleMetadata)])
            
            #devide by sum per sample
            normalized <-  apply(sampleMetadata[,COLLUMN_NR_START_VARIABLES:length(sampleMetadata)], 1, function(x){t(x/sum(x)) }) #iterate over rows =1 (samples), but saves results as column!!! 
            sampleMetadata[,COLLUMN_NR_START_VARIABLES:length(sampleMetadata)] <- t(normalized) #needs to be transposed for correct tic-norm
            
            #must be re-scaled to same order of scale (afrondingsfouten bij kleine getallen tijdens berek PCA, daarom all getallen maal zelfde factor)
            averageTIC_after <- mean(as.matrix(sampleMetadata[,COLLUMN_NR_START_VARIABLES:length(sampleMetadata)]))
            factor <- averageTIC_before/averageTIC_after
            sampleMetadata[,COLLUMN_NR_START_VARIABLES:length(sampleMetadata)] <- factor * sampleMetadata[,COLLUMN_NR_START_VARIABLES:length(sampleMetadata)]
          }
          if(NORMALIZE_METHOD1[ticparam] == NORMALIZE_WITH_ITIC){
            #done by progenisis as 'tic normalisation', iterate over samples
            #= intensity sample 1 / sum(intenstity all samples) for 1 metabolite
            sampleMetadata[,COLLUMN_NR_START_VARIABLES:length(sampleMetadata)] <- apply(sampleMetadata[,COLLUMN_NR_START_VARIABLES:length(sampleMetadata)], 2, function(x) x/sum(x)) #iterate over cols =2 (compids)
          }
          if(NORMALIZE_METHOD1[ticparam] == NORMALIZE_WITH_MAX){
            #'sumarizednormalized' named at lca
            #= intensity sample 1 / max(intenstity all samples) for 1 metabolite
            sampleMetadata[,COLLUMN_NR_START_VARIABLES:length(sampleMetadata)] <- apply(sampleMetadata[,COLLUMN_NR_START_VARIABLES:length(sampleMetadata)], 2, function(x) x/max(x)) #iterate over cols =2 (compids)
          }
          if(NORMALIZE_METHOD1[ticparam] == NORMALIZE_NOT1){
            sampleMetadata <- sampleMetadata #nothing changes
          }
          #write_dataframe_as_txt_file(sampleMetadata, paste0('NORMALIZE_METHOD1_sampleMetadata', combi_nr, '.txt'))
          
          
          ### Normalize samples
          if(NORMALIZE_METHOD[param] == NORMALIZE_WITH_QCs){
            stopifnot("QC" %in% sampleMetadata$Type | "IQC" %in% sampleMetadata$Type)
            samples_metadata <- sampleMetadata[sampleMetadata$Type == 'Sample',]
            QC_metadata <- sampleMetadata[sampleMetadata$Type == 'QC' |sampleMetadata$Type == 'IQC',] ##also allow IQC
            normalized_samples_metadata <- normalize_with_average_QCs(samples_metadata, QC_metadata)
          }
          if(NORMALIZE_METHOD[param] == NORMALIZE_WITH_IQCs){
            stopifnot("QC" %in% sampleMetadata$Type | "IQC" %in% sampleMetadata$Type)
            samples_IQC_metadata <- sampleMetadata[sampleMetadata$Type == 'IQC' | sampleMetadata$Type == 'Sample' |sampleMetadata$Type == 'QC' ,] ##also allow QC
            normalized_samples_metadata <- normalize_with_IQCs(samples_IQC_metadata)  #no info, except sampleName as last collumn
          }
          if(NORMALIZE_METHOD[param] == NORMALIZE_QC_RLSC){
            stopifnot("QC" %in% sampleMetadata$Type | "IQC" %in% sampleMetadata$Type)
            #order qc == 1 in function normalize_QC_RLSC
            normalized_samples_metadata <- normalize_QC_RLSC(sampleMetadata) #need to be in same format = matrix w/o info + no inf/na
          }
          if(NORMALIZE_METHOD[param] == NORMALIZE_NOT){
            samples_metadata <- sampleMetadata[sampleMetadata$Type == 'Sample',]
            normalized_samples_metadata <- normalize_not(samples_metadata) #change nothing but need to be in same format = matrix w/o info + no inf/na
          }
          #write_dataframe_as_txt_file(normalized_samples_metadata, paste0('normalized_samples_matrix', combi_nr, '.txt'))
          
          ### Normalize (I)QCs (for plotting in pca-x instead of OPLSDA) 
          if ("IQC" %in% sampleMetadata$Type | "QC" %in% sampleMetadata$Type){
            QC_metadata <- sampleMetadata[sampleMetadata$Type == 'QC' | sampleMetadata$Type == 'IQC',]
            if(NORMALIZE_METHOD[param] == NORMALIZE_WITH_QCs){
              normalized_QC_metadata <- normalize_with_average_QCs(QC_metadata, QC_metadata)
            }
            if(NORMALIZE_METHOD[param] == NORMALIZE_WITH_IQCs){
              normalized_QC_metadata <- normalize_with_average_QCs(QC_metadata, QC_metadata)
            }
            if(NORMALIZE_METHOD[param] == NORMALIZE_QC_RLSC){
              normalized_QC_metadata <- NULL  #empty because all samples+qc + std+blanks included in loess output samples
            }
            if(NORMALIZE_METHOD[param] == NORMALIZE_NOT){
              normalized_QC_metadata <- normalize_not(QC_metadata)
            }
          }
          if ("IQC" %in% sampleMetadata$Type == FALSE & "QC" %in% sampleMetadata$Type == FALSE){
            normalized_QC_metadata <- normalize_not(QC_metadata)
          }
          
          ### replace intensities with normalized values (both samples + (I)QCs)
          normalized_samples_QC_metadata <- rbind(normalized_samples_metadata, normalized_QC_metadata)
          #write_dataframe_as_txt_file(normalized_samples_QC_metadata, paste0('normalized_samples_QC_matrix', combi_nr, '.txt'))
          
          sampleMetadata_until_start_variables <- subset(sampleMetadata, select = c(1:(COLLUMN_NR_START_VARIABLES-1))) 
          sampleMetadata_samples_QC_until_start_variables <- sampleMetadata_until_start_variables[sampleMetadata$Type == 'Sample' | sampleMetadata$Type == 'QC' | sampleMetadata$Type == 'IQC',]
          
          normalizedMetadata <- merge_accoding_to_SampleName(sampleMetadata_samples_QC_until_start_variables, normalized_samples_QC_metadata)
          #write_dataframe_as_txt_file(normalizedMetadata, paste0('NORMALIZE_METHOD_normalizedMetadata', combi_nr, '.txt'))
          ####################
          
          
          #########skew###########
          ## Check normality of biological samples after normalization
          nsamples_metadata <- normalizedMetadata[normalizedMetadata$Type == 'Sample',] 
          nsamples_matrix <- from_df_to_matrix(nsamples_metadata)
          
          amount_of_variables_with_normal_distribution <- Shapiro_Wilk_test(nsamples_matrix)
          total_amount_of_variables <- ncol(nsamples_matrix)
          percentage <- round(((amount_of_variables_with_normal_distribution/total_amount_of_variables)*100), digits=2)
          
          report_check_normality <- paste("There are " , amount_of_variables_with_normal_distribution, " variables retrieved with a normal distribution from the total of " , total_amount_of_variables , " variables across the samples after normalisation (", percentage, "%). ", combi_nr, sep="")
          append_result_to_report(report_check_normality, name_report_normality)
          
          skewkurtplot <- plot_skewkurt(nsamples_matrix)
          #pdf(paste(name_project,'_skewkurtplot_normalized_samples', combi_nr, '.pdf', sep=""),height=5,width=7)
          #plot(skewkurtplot)
          #dev.off()
          
          png(paste(name_project,'_skewkurtplot_normalized_samples', combi_nr, '.png', sep=""), width=7, height=5, units="in", res=150)
          plot(skewkurtplot)
          dev.off()
          ####################
         
           

          #############loops log, pareto###################
          #save for each combi start with chosen normalisation
          save_normalizedMetadata <- normalizedMetadata

          
          # individual loops per type of hyperparameter (transf, scaling)
          for(transf_param in 1:amount_of_transf_param){
            for(scaling_param in 1:amount_of_scaling_param){
              
              #transf_param <- 1
              #scaling_param <- 1
              #print(combi_nr)
              #print(TRANSFORMATION[transf_param])
              #print(SCALING[scaling_param])
              
              #load from saved 
              normalizedMetadata <- save_normalizedMetadata
              
              ## Log transformation and Pareto scaling for samples + QCs toghether
              if(TRANSFORMATION[transf_param] == TRANSFORMATION_NOT){
                lognormalizedMetadata_samples_QC <- normalizedMetadata #nothing changes
              }
              if(TRANSFORMATION[transf_param] == TRANSFORMATION_LOG){
                lognormalizedMetadata_samples_QC <- log_transformation(normalizedMetadata)
              }
              if(SCALING[scaling_param] == SCALING_NOT){
                scalednormalizedMetadata_samples_QC <- normalizedMetadata #nothing changes
              }
              if(SCALING[scaling_param] == SCALING_PARETO){
                scalednormalizedMetadata_samples_QC <- Pareto_scaling(lognormalizedMetadata_samples_QC)
              }
              #name_df <- paste(name_project,'_normalized, log transformed and pareto scaled samples and QC Metadata', combi_nr, '.txt', sep="")
              #write_dataframe_as_txt_file(scalednormalizedMetadata_samples_QC, name_df)
              
              # cols with value O everywhere before log/pareto are now NA. NA -> 0 for ok sharipo, delete not needed at this time (below @pca are deleted)
              scaledmatrix_samples_QC <- from_df_to_matrix(scalednormalizedMetadata_samples_QC)
              scaledmatrix_samples_QC <- data.frame(sapply(scaledmatrix_samples_QC, function(x) ifelse(is.na(x), 0, x)))
              
              #check normal
              amount_of_variables_with_normal_distribution <- Shapiro_Wilk_test(scaledmatrix_samples_QC)
              total_amount_of_variables <- ncol(scaledmatrix_samples_QC)
              percentage <- round(((amount_of_variables_with_normal_distribution/total_amount_of_variables)*100), digits=2)
              report_check_normality <- paste("There are " , amount_of_variables_with_normal_distribution, " variables retrieved with a normal distribution from the total of " , total_amount_of_variables , " variables across the samples after normalisation, log transformation and pareto scaling of both samples and QCs (", percentage, "%). ", combi_nr, sep="")
              append_result_to_report(report_check_normality, name_report_normality)
              
              skewkurtplot <- plot_skewkurt(scaledmatrix_samples_QC)
              #pdf(paste(name_project,'_skewkurtplot_samples_QCs_scaled_and_log-transformed', combi_nr, '.pdf', sep=""),height=5,width=7)
              #plot(skewkurtplot)
              #dev.off()
              
              png(paste(name_project,'_skewkurtplot_samples_QCs_scaled_and_log-transformed', combi_nr, '.png', sep=""), width=7, height=5, units="in", res=150)
              plot(skewkurtplot)
              dev.off()
              
              ## Log transformation and Pareto scaling for only samples
              nsamples_metadata <- normalizedMetadata[normalizedMetadata$Type == 'Sample',]
              if(TRANSFORMATION[transf_param] == TRANSFORMATION_NOT){
                lognormalizedMetadata_samples <- nsamples_metadata #nothing changes
              }
              if(TRANSFORMATION[transf_param] == TRANSFORMATION_LOG){
                lognormalizedMetadata_samples <- log_transformation(nsamples_metadata)
              }
              if(SCALING[scaling_param] == SCALING_NOT){
                scalednormalizedMetadata_samples <- nsamples_metadata #nothing changes
              }
              if(SCALING[scaling_param] == SCALING_PARETO){
                scalednormalizedMetadata_samples <- Pareto_scaling(lognormalizedMetadata_samples)
              }
              #name_df <- paste(name_project,'_normalized, log transformed and pareto scaled biological samples Metadata', combi_nr, '.txt', sep="")
              #write_dataframe_as_txt_file(scalednormalizedMetadata_samples, name_df)

              
              # cols with value O everywhere before log/pareto are now NA. NA -> 0 for ok sharipo, delete not needed at this time (below @pca are deleted)
              scaledmatrix_samples <- from_df_to_matrix(scalednormalizedMetadata_samples)
              scaledmatrix_samples <- data.frame(sapply(scaledmatrix_samples, function(x) ifelse(is.na(x), 0, x)))
              
              #check normal
              amount_of_variables_with_normal_distribution <- Shapiro_Wilk_test(scaledmatrix_samples)
              total_amount_of_variables <- ncol(scaledmatrix_samples)
              percentage <- round(((amount_of_variables_with_normal_distribution/total_amount_of_variables)*100), digits=2)
              report_check_normality <- paste("There are " , amount_of_variables_with_normal_distribution, " variables retrieved with a normal distribution from the total of " , total_amount_of_variables , " variables across the samples after normalisation, log transformation and pareto scaling of samples (", percentage, "%). ", combi_nr, sep="")
              append_result_to_report(report_check_normality, name_report_normality)
              
              skewkurtplot <- plot_skewkurt(scaledmatrix_samples)
              #pdf(paste(name_project,'_skewkurtplot_samples_scaled_and_log-transformed', combi_nr, '.pdf', sep=""),height=5,width=7)
              #plot(skewkurtplot)
              #dev.off()
              
              png(paste(name_project,'_skewkurtplot_samples_scaled_and_log-transformed', combi_nr, '.png', sep=""), width=7, height=5, units="in", res=150)
              plot(skewkurtplot)
              dev.off()
              ####################
              
              
              
              ##########PCA 1 projection##########
              ## PCA for projection 
              projection <- 1   #!! always calculate KNN with 1st projection
            
              print(paste("start calculation PCA ", projection, "-", combi_nr, sep=""))
              
              ### remove non-essential data for comparison
              #remove samples that are not included in comp (Na instead of 0/1)
              nsamples_metadata <- scalednormalizedMetadata_samples_QC #also QC can be in PCA
              comp_ <- nsamples_metadata[,COLLUMN_NR_PROJECTION1+projection-1]
              stopifnot(class(comp_)=='integer') #need {0,1,2, ...} arguments, rest is NA and not ""!
              samples_metadata_comp <- nsamples_metadata[!is.na(comp_), ]    
              
              #remove variables that do not occur in comparison (not detected in both groups); value NaN after scaling
              samples_matrix_comp <- from_df_to_matrix(samples_metadata_comp)
              #samples_matrix_comp_no0 <- samples_matrix_comp[, !apply(samples_matrix_comp == 0, 2, all)]  #remove 0
              samples_matrix_comp_no0 <- data.frame(sapply(samples_matrix_comp, function(x) ifelse(is.nan(x), NA, x)))
              samples_matrix_comp_no0 <- samples_matrix_comp_no0[,which(unlist(lapply(samples_matrix_comp_no0, function(x)!all(is.na(x)))))]
              
              comp <- as.factor(samples_metadata_comp[,COLLUMN_NR_PROJECTION1+projection-1])
              
              ### PCA for each projection
              suppressMessages(library(pcaMethods))
              if(PC_AMOUNT == DEFAULT_PC_AMOUNT){
                number_samples <- nrow(samples_matrix_comp_no0)     #Amount of samples
                number_samples <- number_samples-1
              }
              if(PC_AMOUNT == FIXED_PC_AMOUNT){
                number_samples = FIXED_PC_AMOUNT
                if(number_samples >= nrow(samples_matrix_comp_no0)){
                  stop("ERROR: Part II: multivariate analysis stopped because number of PCs exceeds number of samples-1.")
                }
              }
              
              #max 10pcs, even if default:
              if(number_samples > 30){
                number_samples <- 30
              }
      
              try({ 
                PCA_comp <- pca(samples_matrix_comp_no0,nPcs=number_samples,scale="none",completeObs=TRUE,subset=NULL,cv="q2",center=FALSE) #Calculate full PCA model (max PCs is # of samples - 1)
                #plot(PCA_comp)   #Rule of thumb: amount of PCs to choose: Where Q2 (calcul by cross validation) is max. => use this (Other rule of thumb: total var. explained +-80%)
                #name_plot <- paste(name_project, "_PCA projection " , projection, combi_nr, ".pdf", sep="")
                #pdf(name_plot, height=5,width=7)
                #plot(PCA_comp)
                #dev.off()
                
                png( paste(name_project, "_PCA projection " , projection, "-", combi_nr, ".png", sep=""), width=7, height=5, units="in", res=150)
                plot(PCA_comp)
                dev.off()
                
                ##############faster, do with first 2 PCs!!!!##############
                #set.seed(42) #Set seed of random number generator for reproducible results
                #Q2CV <- Q2(PCA_comp,originalData = completeObs(PCA_comp),fold=7,nruncv = 5,type="krzanowski") #Choose optimal number of components from this
                #Q2max <- max(Q2CV[,5])
                #nPCsopt <- which(grepl(Q2max, Q2CV[,5])) #nr of row (row1 == pc1)    #faster, do with first 2 PCs!!!!
                #if(nPCsopt == 1){
                nPCsopt <- 2
                #}
                #################
                  
                PCAoptnumber <- pca(samples_matrix_comp_no0,nPcs=nPCsopt,scale="none",completeObs=TRUE,subset=NULL,cv="q2",center=FALSE)
                PCAscores_comp <- as.data.frame(PCAoptnumber@scores)
                
                HotellingellipsePCA_comp <- Hotellingellipse(number_samples,PCAscores_comp)  #Calculate Hotellis ng'T2 ellipse for the PCA
                dev.off()
                PCAplot_comp <- plot_pcascores(PCAscores_comp, comp) 
                #PCAplot_comp
                #name_plot <- paste(name_project, "_PCA_scoreplot " , projection, combi_nr, ".pdf", sep="")
                #pdf(name_plot) #, height=5,width=7)
                #plot(PCAplot_comp)
                #dev.off()
                
                png(paste(name_project, "_PCA_scoreplot " , projection, "-", combi_nr, ".png", sep="")) #, width=7, height=5, units="in", res=150)
                plot(PCAplot_comp)
                dev.off()
                
                #pca score plot weight values
                score_weights_pca_comp <- PCAscores_comp[ ,1:2] #PC1 and PC2 are kept
                rownames(score_weights_pca_comp) <- samples_metadata_comp$SampleName
                name_df <- paste(name_project, '_score_weights_projection_', projection, combi_nr, '.txt', sep="")
                write_matrix_as_txt_file(score_weights_pca_comp, name_df) 
                #plot(score_weights_pca_comp)
              })
                
              print(paste("finished calculation PCA ", projection, "-", combi_nr, sep=""))
            
              ###################
              
                
              #########Score with KNN##########
              ### score each combination using KNN (! << testset kleinste groep # neighbors):
              #https://towardsdatascience.com/k-nearest-neighbors-algorithm-with-examples-in-r-simply-explained-knn-1f2c88da405c
          
              #score_weights_pca_comp <- PCAscores_comp[ ,1:2] #for testing
              
              #pca score plot weight values (start with this from projections write to txt scores, only 2PCs) #todo: now scores + comp info from last PCA projection in loop above...
              score_weights_pca_comp$Group <- comp
              score_weights_pca_comp$Group <- as.numeric(score_weights_pca_comp$Group)
              score_weights_pca_comp$Group[is.na(score_weights_pca_comp$Group)] <- 0 #no NAs in knn allowed
              
              ##cross-validata 7times:
              scores <- NULL
              max_crossval <- 7
              
              for(crossval in 1:max_crossval){
                #Generate a random number that is 70% of the total number of rows in dataset.
                ran <- sample(1:nrow(score_weights_pca_comp), 0.7 * nrow(score_weights_pca_comp)) 
                
                train <- score_weights_pca_comp[ran,1:2] #col of 2 pcs, part 1-100
                test <- score_weights_pca_comp[-ran,1:2] #92 of part 2
                train_category <- score_weights_pca_comp$Group[ran] #part 1-100 of train
                test_category <- score_weights_pca_comp$Group[-ran]
                
                ##PCA-KNN
                #library(class) ##run knn function
                #KNN_NR <- min(summary(comp))-1 #amount of smallest group in comp -1 so odd number
                #if(KNN_NR %% 2 == 0){
                #  KNN_NR <- KNN_NR-1 #if even: -2 so odd number
                #} 
                #pred <- knn(as.matrix(train),as.matrix(test),cl=as.matrix(train_category),k=KNN_NR) #/search 4 nearest neighors
                #pred <- as.character(pred)
                #length(pred)
                
                
                ##PCA-SVM 
                #library(e1071)
                #https://www.datacamp.com/community/tutorials/support-vector-machines-r
                #x <- as.matrix(train)
                #y <- as.matrix(train_category)
                #y <- as.factor(y)
                #model <- svm(x, y, kernel = "linear") 
                #print(model)
                #summary(model)
                #pred <- predict(model, as.matrix(test))
                #pred <- as.character(pred)

                
                ##PCA-LDA
                library(MASS)
                y <- as.matrix(train_category)
                y <- as.factor(y)
                model <- lda(y ~ ., data = train) #train is df
                pred <- predict(model, test) #test is df
                pred <- as.character(pred$class)
                #length(pred)
                
                
                ##make confusion matrix
                tab <- table(pred,test_category)
                
                
                
                ##score using WITH qc's, calc accuracy only for QC samples in Projection1
                if("QC" %in% samples_metadata_comp$Type |"IQC" %in% samples_metadata_comp$Type){
                  
                  #find number in projection1 of the QCs
                  subset_QCS <- samples_metadata_comp[samples_metadata_comp$Type == "QC" |samples_metadata_comp$Type == "IQC" ,]
                  nr_QC_projection1 <- subset_QCS$Projection1
                  
                  #stop if QCs in more than 1 group in Projection1
                  if(sd(nr_QC_projection1) > 0){
                    stop("Error: you can only have 1 group defined as QCs in Projection1")
                  }
                  
                  #QC is nr in projection1:
                  nr_QC_projection1 <- mean(nr_QC_projection1) #only 1 number needed
                  #in not all groups in test_category and/or pred: incomplete matrix 5*6 bv => find based on row and col name by using "3" ipv 3
                  nr_QC_projection1 <- as.character(nr_QC_projection1) #as string for column name in tab
                  
                  #if no pred/test_category contains the QC as group, next crossval + print (ok?):
                  if(!nr_QC_projection1 %in% rownames(tab)){
                    print(paste0("Remark: no prediction possible with group of QCs present in crossval ", crossval, " of ", max_crossval, " for combi_nr ", combi_nr))
                    next
                  }
                  
                  ##change confusion matrix with adjusted weigths
                  tab_weighted <- as.data.frame.matrix(tab) 
                  
                  #on row of QC group predicted: if good: leave, if bad each bad*F
                  value_QC_corr <- tab_weighted[nr_QC_projection1,nr_QC_projection1] #row first, quoted column name second, eg 3,3
                  
                  tab_weighted <- as.matrix(tab_weighted[nr_QC_projection1,])
                  score <- value_QC_corr/sum(tab_weighted)*100
                  
                }  
                
                ##score using crossval WO qc's, so same weight for each group in Projection1
                if(!("QC" %in% samples_metadata_comp$Type |"IQC" %in% samples_metadata_comp$Type)){
                  
                  #calc score
                  accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
                  #for df add score
                  score <- accuracy(tab) #% correct
                  #print(paste0(KNN_NR, " knn's -> accuracy: ", score))
                }
                
                PCs_QCs <- score_weights_pca_comp[score_weights_pca_comp$Group == nr_QC_projection1,]
                PC1_QCs <- mean(PCs_QCs$PC1)
                PC2_QCs <- mean(PCs_QCs$PC2)
                score <- PC2_QCs
                #since have to rewrite all, work with score op pc1 (number, so averae cv etc.)
                scores <- c(scores, score)
              }
              
              #score of knn (average)
              average_score <- mean(scores)
              
              #append df
              report_hyperparameter_test_combi <- NULL
              report_hyperparameter_test_combi$combination <- combi_nr
              report_hyperparameter_test_combi$QC_variance <- as.character(COEFICIENT_OF_VARIANCE[filter1])
              report_hyperparameter_test_combi$QC_presence <- as.character(QC_PREFILTER[filter2])
              report_hyperparameter_test_combi$IS_normalisation <- as.character(NORMALIZE_METHOD0[isparam])
              report_hyperparameter_test_combi$TIC_normalisation <- as.character(NORMALIZE_METHOD1[ticparam])
              report_hyperparameter_test_combi$QC_normalisation <- as.character(NORMALIZE_METHOD[param])
              report_hyperparameter_test_combi$Transformation <- as.character(TRANSFORMATION[transf_param])
              report_hyperparameter_test_combi$Scaling <- as.character(SCALING[scaling_param])
              #report_hyperparameter_test_combi$neighbours_nr <- KNN_NR
              report_hyperparameter_test_combi$score <- average_score
              report_hyperparameter_test_combi <- as.data.frame(report_hyperparameter_test_combi)
              
              report_hyperparameter_test <- rbind(report_hyperparameter_test, report_hyperparameter_test_combi)
              ###################
              
              
              combi_nr <- combi_nr + 1
            }
          }
        }
      }
    }
  }
}

#write resutls KNN scoring
write_dataframe_as_txt_file(report_hyperparameter_test, paste0(name_project,'_report_hyperparameter_test.txt'))

#best hyperparameter combination
optimal_combination <- report_hyperparameter_test[which.max(report_hyperparameter_test$score),] #row select with max score = accuraty correct/false
write_dataframe_as_txt_file(optimal_combination, paste0(name_project,'_report_optimal_combination.txt'))

#select best and save for part2: statistical analysis
COEFICIENT_OF_VARIANCE <- optimal_combination[2] 
QC_PREFILTER <- optimal_combination[3]
NORMALIZE_METHOD0 <- optimal_combination[4] 
NORMALIZE_METHOD1 <- optimal_combination[5] #4e col is tic 
NORMALIZE_METHOD <- optimal_combination[6] #5e col is qc
TRANSFORMATION <- optimal_combination[7]
SCALING <- optimal_combination[8]


### zip results folder
if (CODE_RUN_MODE == CODE_AUTORUN){
  files2zip <- dir(path_data_out_param, full.names = FALSE)
  zip(zipfile = 'paramZip', files = files2zip)
  file.remove(files2zip)
  write_dataframe_as_txt_file(report_hyperparameter_test, paste0(name_project,'_report_hyperparameter_test.txt')) #copy outside zip
  setwd(path_data_out)
  write_dataframe_as_txt_file(optimal_combination, paste0(name_project,'_report_optimal_combination.txt')) #set copy in output for reporting
}


print("R pipeline - hyperparameter optimization statistics - done!")
print(Sys.time())
end_time <- Sys.time()
print(end_time - start_time)
#
#####################

